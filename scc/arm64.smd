// SPDX-License-Identifier: MIT

:	reg

//// register
//regw:	REG		== %r0 // if width == 32
//regx:	REG		== %r0 // if width == 64
reg:	REG		== %r0
reg:	ARG		== %r0

reg0:	reg		== %r0
reg0:	ZERO		== %rz

//// constants
imm12:	CONST		// if range(0, 4095)
imm12:	ZERO

imm16:	CONST		// if range(0, 65535)
imm16:	ZERO		//

bimm:	CONST		// if "bitmask"

bit:	CONST		// if == 1 << sh

one:	CONST		// if == 1
nzero:	CONST		// if == -1

zero:	ZERO
const:	CONST

// for now those cannot work: a pseudo is either PSEUDO_VAL or PSEUDO_REG
// the following is just some hackery to make it seems it is working
creg:	CONST			[4] => ldr	%rt, =%c0
creg:	CONST			[2] => movw	%rt, #:lower16:%c0;  movt	%rt, #:upper16:%c0
reg:	COPY(const)		[2] => movw	%rd, #:lower16:%c0;  movt	%rt, #:upper16:%c0
creg:	imm16			[1] => mov	%rt, %c0
creg:	ZERO			[1] => mov	%rt, #0
reg:	creg			== %rt

//// move
//movn
//movz
//movk

reg:	COPY.L(zero)		[1] => mov	%rd, wzr
reg:	COPY.Q(zero)		[1] => mov	%rd, xzr
reg:	COPY(imm16)		[1] => mov	%rd, %c1
reg:	COPY(reg)		[1] => mov	%rd, %r1

				// 'adr' is enough if label within +- 1MB
reg:	SETVAL			[2] => adrp	%rt, %x; add	%rt, %rt, :lo12:%x


//// shifted register
sh:	CONST		== %c0 // if range(0, 31) or range(0, 63)
sreg:	reg		== %r0
sreg:	SHL(reg, sh)		[1] == %r1 LSL %c2
sreg:	LSR(reg, sh)		[1] == %r1 LSR %c2
sreg:	ASR(reg, sh)		[1] == %r1 ASR %c2
rreg:	reg		== %r0
rreg:	SHL(reg, sh)		[1] == %r1 LSL %c2
rreg:	LSR(reg, sh)		[1] == %r1 LSR %c2
rreg:	ASR(reg, sh)		[1] == %r1 ASR %c2
rreg:	ROR(reg, sh)		[1] == %r1 ROR %c2

//// extended register
xsh:	CONST		== %c0 // if range(0, 4)
xt:	ZEXTB(reg)		    == %r1 UXTB
xt:	ZEXTH(reg)		    == %r1 UXTH
xt:	ZEXTL(reg)		    == %r1 UXTW
xt:	ZEXTQ(reg)		    == %r1 UXTX
xt:	SEXTB(reg)		    == %r1 SXTB
xt:	SEXTH(reg)		    == %r1 SXTH
xt:	SEXTL(reg)		    == %r1 SXTW
xt:	SEXTQ(reg)		    == %r1 SXTX
xreg:	SHL(reg,xsh)		[0] == %r1 LSL %c2
xreg:	SHL(xt, xsh)		[0] == %a1 %c2
xreg:	xt			[0] == %a1

//// any register mode
areg:	reg
areg:	xreg
areg:	sreg

//// arithmetic
reg:	ADD(reg, imm12)		[1] => add	%rd, %r1, %c2
reg:	ADD(reg, areg)		[1] => add	%rd, %r1, %a2
reg:	ADD(areg, reg)		[1] => add	%rd, %r2, %a1
reg:	SUB(reg, imm12)		[1] => sub	%rd, %r1, %c2
reg:	SUB(reg, areg)		[1] => sub	%rd, %r1, %r2
reg:	NEG(sreg)		[1] => neg	%rd, %a1

//// multiply/divide
// cost 3/5 for 32/64 bit
reg:    MUL(reg, imm12)		[9] => mul	%rd, %r1, %r2	// FIXME
reg:    MUL(reg, reg)		[5] => mul	%rd, %r1, %r2
reg:    MUL(reg, ADD(reg,reg))	[5] => madd	%rd, %r1, %r21, %r22
reg:    MUL(reg, SUB(reg,reg))	[5] => msub	%rd, %r1, %r21, %r22
reg:    MUL(reg, NEG(reg))	[5] => mneg	%rd, %r1, %r21

// cost 4-20/36 for 32/64 bit
reg:    DIVS(reg, reg)		[36] => sdiv	%rd, %r1, %r2
reg:    DIVU(reg, reg)		[36] => udiv	%rd, %r1, %r2
reg:    MODS(reg, reg)		[41] => sdiv	%rd, %r1, %r2; msub	%rd, %rd, %r2, %r1
reg:    MODU(reg, reg)		[41] => udiv	%rd, %r1, %r2; msub	%rd, %rd, %r2, %r1

//// logical
nrreg:	NOT(rreg)		== %a1
reg:	NOT(rreg)		[1] => mvn	%rd, %a1
reg:	AND(reg, bimm)		[1] => and	%rd, %r1, %c2
reg:	AND(reg, rreg)		[1] => and	%rd, %r1, %a2
reg:	AND(rreg, reg)		[1] => and	%rd, %r2, %a1
reg:	AND(reg, nrreg)		[1] => bic	%rd, %r1, %a2
reg:	AND(nrreg, reg)		[1] => bic	%rd, %r2, %a1
reg:	OR(reg, bimm)		[1] => orr	%rd, %r1, %c2
reg:	OR(reg, rreg)		[1] => orr	%rd, %r1, %a2
reg:	OR(rreg, reg)		[1] => orr	%rd, %r2, %a1
reg:	OR(reg, nrreg)		[1] => orn	%rd, %r1, %a2
reg:	OR(nrreg, reg)		[1] => orn	%rd, %r2, %a1
reg:	XOR(reg, bimm)		[1] => eor	%rd, %r1, %c2
reg:	XOR(reg, rreg)		[1] => eor	%rd, %r1, %a2
reg:	XOR(rreg, reg)		[1] => eor	%rd, %r2, %a1
reg:	XOR(reg, nrreg)		[1] => eon	%rd, %r1, %a2
reg:	XOR(nrreg, reg)		[1] => eon	%rd, %r2, %a1

//// shift/rotate
sh:	CONST		// FIXME
reg:	SHL(reg, sh)		[1] => lsl	%rd, %r1, %c2
reg:	SHL(reg, reg)		[1] => lslv	%rd, %r1, %r2
reg:	LSR(reg, sh)		[1] => lsr	%rd, %r1, %c2
reg:	LSR(reg, reg)		[1] => lsrv	%rd, %r1, %r2
reg:	ASR(reg, sh)		[1] => asr	%rd, %r1, %c2
reg:	ASR(reg, reg)		[1] => asrv	%rd, %r1, %r2
reg:	ROR(reg, sh)		[1] => ror	%rd, %r1, %c2
reg:	ROR(reg, reg)		[1] => rorv	%rd, %r1, %r2

//// sign/zero extension & truncation
reg:	ZEXTB(reg)		[1] => uxtb	%rd, %r1
reg:	ZEXTH(reg)		[1] => uxth	%rd, %r1
reg:	ZEXTL(reg)		[1] => uxtw	%rd, %r1
reg:	SEXTB(reg)		[1] => sxtb	%rd, %r1
reg:	SEXTH(reg)		[1] => sxth	%rd, %r1
reg:	SEXTL(reg)		[1] => sxtw	%rd, %r1

reg:	TRUNC(reg, sh)		[1] => and	%rd, %r1, #((1 << %c2) - 1)

//// bitfield extraction/insertion
//=> bfi
//=> bfxil

//sbfx %rd, %r1, #lsb, #width	== sbfm %rd, %r1, #lsb, #lsb+width-1
//sxtb %rd, %r1			== sbfm %rd, %r1, #0, #7
//sxth %rd, %r1			== sbfm %rd, %r1, #0, #15
//sxtw %rd, %r1			== sbfm %rd, %r1, #0, #31

//ubfx %rd, %r1, #lsb, #width	== ubfm %rd, %r1, #lsb, #lsb+width-1
//uxtb %rd, %r1			== ubfm %rd, %r1, #0, #7
//uxth %rd, %r1			== ubfm %rd, %r1, #0, #15

reg:	CAST(reg, sh)		[1] => ubfx	%rd, %r1, #0, %c2
reg:	SCAST(reg, sh)		[1] => sbfx	%rd, %r1, #0, %c2
reg:	CAST(LSR(reg, sh), sh)	[1] => ubfx	%rd, %r1, %c12, %c2
reg:	SCAST(LSR(reg, sh), sh)	[1] => sbfx	%rd, %r1, %c12, %c2
reg:	ZEXTB(LSR(reg, sh))	[1] => ubfx	%rd, %r1, %c12, #8
reg:	SEXTB(LSR(reg, sh))	[1] => sbfx	%rd, %r1, %c12, #8
reg:	ZEXTH(LSR(reg, sh))	[1] => ubfx	%rd, %r1, %c12, #16
reg:	SEXTH(LSR(reg, sh))	[1] => sbfx	%rd, %r1, %c12, #16
reg:	ZEXTL(LSR(reg, sh))	[1] => ubfx	%rd, %r1, %c12, #32
reg:	SEXTL(LSR(reg, sh))	[1] => sbfx	%rd, %r1, %c12, #32

//// load and store
//*XT* == LSL, UXTW, SXTW, SXTX
//ldp   wt1, wt2, [Xn], #simm7 * 4
//ldpsw wt1, wt2, [Xn], #simm7 * 4; // wt1 & wt2 are then sign extended
//ldp   xt1, xt2, [Xn], #simm7 * 8

//ldurb  wt, [Xn, #simm9]
//ldursb rt, [Xn, #simm9]
//ldurh  wt, [Xn, #simm9]
//ldursh rt, [Xn, #simm9]
//ldur   rt, [Xn, #simm9]
//ldursw xt, [Xn, #simm9]

//ldr   rt, label
//ldrsw xt, label

// FIXME: need to add pre and post writeback modes

asym:	GSYM			[1] => adrp	%rt, %l0; add	%rt, %rt, :lo12:%l0
asym:	LSYM			[1] => add	%rt, sp, SP@%c0
reg:	asym			    == %rp	// this make a PSEUDO_REG from a _SYM
addr:	asym			    == %rp

addr:	reg			    == %r0
addr:	ADD(reg, imm12)		    == %r1, %c2
addr:	ADD(reg, axreg)		    == %r1, %a2
addr:	ADD(reg, sreg)		    == %r1, %a2
axreg:	reg			    == %r0
axreg:	ZEXTL(reg)		    == %r1, uxtw
axreg:	SEXTL(reg)		    == %r1, sxtw
axreg:	SHL(sxreg, sh)		    == %a1 #%c2
sxreg:	ZEXTL(reg)		    == %r1, uxtw
sxreg:	SEXTL(reg)		    == %r1, sxtw

reg:	LOAD.Q(addr)		[4] => ldr	%rd, [%a1]

reg:	LOAD.L(addr)		[4] => ldr	%rd, [%a1]
ldl:	LOAD.L(addr)		    == %a1
reg:	ZEXTL(ldl)		[4] => ldr	%rd, [%a1]
reg:	SEXTL(ldl)		[4] => ldrsw	%rd, [%a1]

reg:	LOAD.H(addr)		[4] => ldrh	%rd, [%a1]
ldh:	LOAD.H(addr)		    == %a1
reg:	ZEXTH(ldh)		[4] => ldrh	%rd, [%a1]
reg:	SEXTH(ldh)		[4] => ldrsh	%rd, [%a1]

reg:	LOAD.B(addr)		[4] => ldrb	%rd, [%a1]
ldb:	LOAD.B(addr)		    == %a1
reg:	ZEXTB(ldb)		[4] => ldrb	%rd, [%a1]
reg:	SEXTB(ldb)		[4] => ldrsb	%rd, [%a1]


:	STORE.Q(addr, reg)	[1] => str	%r2, [%a1]
:	STORE.L(addr, reg)	[1] => str	%r2, [%a1]
:	STORE.H(addr, reg)	[1] => strh	%r2, [%a1]
:	STORE.B(addr, reg)	[1] => strb	%r2, [%a1]

reg:	LOADW2(reg)		[4] => ldp	%r1, %rd:lo, %rd:up
:	STOREW2(reg, reg)	[1] => stp	%r1, %r2:lo, %r2:up

:	STOREMEM(reg, LOADMEM(reg), const) [30]=> bl	memcpy(%r1, %r2.1, %c3)

//// compare
// comp(reg, xreg)	=> cmn
// comp(reg, imm12)	=> cmn
// comp(reg, rreg)	=> cmn
// comp(reg, NEG(xreg))	=> cmn
// comp(reg, -imm12))	=> cmn
// comp(reg, NEG(rreg))	=> cmn

tsteq:	reg			[1] => cmp	%r0, #0
tstne:	reg			[1] => cmp	%r0, #0

tsteq:	SET_EQ(reg, areg)	[1] => cmp	%r1, %r2
tsteq:	SET_EQ(reg, imm12)	[1] => cmp	%r1, %c2
tstne:	SET_NE(reg, areg)	[1] => cmp	%r1, %r2
tstne:	SET_NE(reg, imm12)	[1] => cmp	%r1, %c2

tstlt:	SET_LT(reg, areg)	[1] => cmp	%r1, %r2
tstlt:	SET_LT(reg, imm12)	[1] => cmp	%r1, %c2
tstle:	SET_LE(reg, areg)	[1] => cmp	%r1, %r2
tstle:	SET_LE(reg, imm12)	[1] => cmp	%r1, %c2
tstgt:	SET_GT(reg, areg)	[1] => cmp	%r1, %r2
tstgt:	SET_GT(reg, imm12)	[1] => cmp	%r1, %c2
tstge:	SET_GE(reg, areg)	[1] => cmp	%r1, %r2
tstge:	SET_GE(reg, imm12)	[1] => cmp	%r1, %c2

tstlo:	SET_B(reg, areg)	[1] => cmp	%r1, %r2
tstlo:	SET_B(reg, imm12)	[1] => cmp	%r1, %c2
tstls:	SET_BE(reg, areg)	[1] => cmp	%r1, %r2
tstls:	SET_BE(reg, imm12)	[1] => cmp	%r1, %c2
tsthi:	SET_A(reg, areg)	[1] => cmp	%r1, %r2
tsthi:	SET_A(reg, imm12)	[1] => cmp	%r1, %c2
tsths:	SET_AE(reg, areg)	[1] => cmp	%r1, %r2
tsths:	SET_AE(reg, imm12)	[1] => cmp	%r1, %c2

reg:	AND(SET_NE(reg,zero),SET_NE(reg,zero))	[3] => cmp	%r11, 0; ccmp	%r21, 0, 4, ne; cset	%rd, ne
reg:	AND(SET_NE(reg,zero),SET_EQ(reg,zero))	[3] => cmp	%r11, 0; ccmp	%r21, 0, 0, ne; cset	%rd, eq
reg:	AND(SET_EQ(reg,zero),SET_NE(reg,zero))	[3] => cmp	%r11, 0; ccmp	%r21, 0, 4, eq; cset	%rd, ne
reg:	AND(SET_EQ(reg,zero),SET_EQ(reg,zero))	[3] => orr	%rd, %r11, %r21; cmp	%rd, 0; cset	%rd, eq
reg:	 OR(SET_NE(reg,zero),SET_NE(reg,zero))	[3] => orr	%rd, %r11, %r21; cmp	%rd, 0; cset	%rd, ne
reg:	 OR(SET_NE(reg,zero),SET_EQ(reg,zero))	[3] => cmp	%r11, 0; ccmp	%r21, 0, 4, eq; cset	%rd, eq
reg:	 OR(SET_EQ(reg,zero),SET_NE(reg,zero))	[3] => cmp	%r11, 0; ccmp	%r21, 0, 0, ne; cset	%rd, ne
reg:	 OR(SET_EQ(reg,zero),SET_EQ(reg,zero))	[3] => cmp	%r11, 0; ccmp	%r21, 0, 4, ne; cset	%rd, eq

// tst %rd, bimm
// tst %rd, rreg

condt:	tsteq		== eq
condt:	tstne		== ne
condt:	tstlt		== lt
condt:	tstle		== le
condt:	tstgt		== gt
condt:	tstge		== ge
condt:	tstlo		== lo
condt:	tstls		== ls
condt:	tsthi		== hi
condt:	tsths		== hs

condf:	tsteq		== ne
condf:	tstne		== eq
condf:	tstlt		== ge
condf:	tstle		== gt
condf:	tstgt		== le
condf:	tstge		== lt
condf:	tstlo		== hs
condf:	tstls		== hi
condf:	tsthi		== ls
condf:	tsths		== lo

:	CBR(condt)		[1] => b.%a1	%b

cbne:	SET_NE(reg, zero)	    == %r1
cbeq:	SET_EQ(reg, zero)	    == %r1
:	CBR(cbne)		[1] => cbnz	%a1, %b
:	CBR(cbeq)		[1] => cbz	%a1, %b

tbeq:	SET_EQ(AND(reg, bit), zero)	    == %r1
tbne:	SET_NE(AND(reg, bit), zero)	    == %r1
//tbxy:	SET_XY(AND(reg, bit), bit)	    == %r1	// if %c12 == %c2
:	CBR(tbne)		[1] => tbnz	%a1, %b
:	CBR(tbeq)		[1] => tbz	%a1, %b

:	BR			[1] => b	%b
:	COMPUTEDGOTO(reg)	[1] => bx	%rd

//// select
reg:	condt				[1] => cset	%rd, %a0

reg:	SEL(condt, reg0, reg0)		[1] => csel	%rd, %r2, %r3, %a1

reg:	SEL(condt, reg0, ADD(reg, one))	[1] => csinc	%rd, %r2, %r31, %a1
reg:	SEL(condf, ADD(reg, one), reg0)	[1] => csinc	%rd, %r21, %r3, %a1
reg:	SEL(condt, reg0, one)		[1] => csinc	%rd, %r21, %rz, %a1
reg:	SEL(condf, one, reg0)		[1] => csinc	%rd, %r3, %rz, %a1

reg:	SEL(condt, reg0, NOT(reg))	[1] => csinv	%rd, %r2, %r31, %a1
reg:	SEL(condf, NOT(reg), reg0)	[1] => csinv	%rd, %r21, %r3, %a1
reg:	SEL(condt, reg0, nzero)		[1] => csinv	%rd, %r2, %rz, %a1
reg:	SEL(condf, nzero, reg0)		[1] => csinv	%rd, %r3, %rz, %a1

reg:	SEL(condt, reg0, NEG(reg))	[1] => csneg	%rd, %r2, %r31, %a1
reg:	SEL(condf, NEG(reg), reg0)	[1] => csneg	%rd, %r21, %r3, %a1


reg:	CALL			[1] => bl	%l1 -> %rd
reg:	INDCALL(reg)		[1] => blr	%r1 -> %rd
tcall:	CALL			[1] => b	%l1
tcall:	INDCALL(reg)		[1] => br	%r1

:	RET(tcall)		[0] == %a1	// premature tail call optimization
:	RET(reg)		[1] => ret	(-> %r1)
:	RETVOID			[1] => ret

////////////////////////////////////////////////////////////////////////
//// floating-point

freg:	ARG
freg:	REG

:	freg
:	RET(freg)		[1] => ret	%r1

freg:	SETFVAL			[2] 		// FIXME
freg:	COPY(freg)		[1] => fmv	%rd, %r1
freg:	FPCAST(reg)		[3] => fcvt.d.w		%rd, %r1
freg:	FPCAST(reg)		[3] => fcvt.d.wu	%rd, %r1
//freg:	FPCAST.S(freg)		[3] => fcvt.d.s		%rd, %r1
//freg:	FPCAST.D(freg)		[3] => fcvt.s.d		%rd, %r1

freg:	FNEG(freg)		[3] => fneg	%rd, %r1
//freg:	FABS(freg)		[3] => fabs	%rd, %r1
freg:	FADD(freg, freg)	[5] => fadd	%rd, %r1, %r2
freg:	FSUB(freg, freg)	[5] => fsub	%rd, %r1, %r2
freg:	FMUL(freg, freg)	[5] => fmul	%rd, %r1, %r2
freg:	FDIV(freg, freg)	[32] => fdiv	%rd, %r1, %r2
//freg:	FMIN(freg, freg)	[5] => fmin	%rd, %r1, %r2
//freg:	FMAX(freg, freg)	[5] => fmax	%rd, %r1, %r2
//freg:	FSQRT(freg)		[32] => fsqrt	%rd, %r1

freg:	FNEG(FMUL(freg, freg))	[5] => fnmul	%rd, %r11, %r12
freg:	FMUL(FNEG(freg), freg)	[5] => fnmul	%rd, %r11, %r2
freg:	FMUL(freg, FNEG(freg))	[5] => fnmul	%rd, %r1, %r21

freg:	FADD(FMUL(freg, freg), freg)	[4] => fmadd	%rd, %r11, %r12, %r2
freg:	FADD(freg, FMUL(freg, freg))	[4] => fmadd	%rd, %r21, %r22, %r1
freg:	FSUB(freg, FMUL(freg, freg))	[4] => fmsub	%rd, %r21, %r22, %r1
freg:	FSUB(FMUL(freg, freg), freg)	[4] => fnmsub	%rd, %r11, %r12, %r2
freg:	FADD(FMUL(FNEG(freg),freg),freg)[4] => fnmsub	%rd, %r111, %r12, %r2
freg:	FSUB(FMUL(FNEG(freg),freg),freg)[4] => fnmadd	%rd, %r111, %r12, %r2
freg:	FSUB(FNEG(freg),FMUL(freg,freg))[4] => fnmadd	%rd, %r21, %r22, %r11
freg:	FNEG(FADD(FMUL(freg,freg),freg))[4] => fnmadd	%rd, %r111, %r112, %r12
freg:	FNEG(FADD(freg,FMUL(freg,freg)))[4] => fnmadd	%rd, %r121, %r122, %r11

tstoeq:	FCMP_OEQ(freg, freg)	[3] => fcmp	%r1, %r2
tstune:	FCMP_UNE(freg, freg)	[3] => fcmp	%r1, %r2
tstolt:	FCMP_OLT(freg, freg)	[3] => fcmp	%r1, %r2
tstole:	FCMP_OLE(freg, freg)	[3] => fcmp	%r1, %r2
tstogt:	FCMP_OGT(freg, freg)	[3] => fcmp	%r1, %r2
tstoge:	FCMP_OGE(freg, freg)	[3] => fcmp	%r1, %r2
tstult:	FCMP_ULT(freg, freg)	[3] => fcmp	%r1, %r2
tstule:	FCMP_ULE(freg, freg)	[3] => fcmp	%r1, %r2
tstugt:	FCMP_UGT(freg, freg)	[3] => fcmp	%r1, %r2
tstuge:	FCMP_UGE(freg, freg)	[3] => fcmp	%r1, %r2

condt:	tstoeq == eq
condt:	tstune == ne
condt:	tstolt == mi
condt:	tstole == ls
condt:	tstogt == gt
condt:	tstoge == ge

condt:	tstult == lt
condt:	tstule == le
condt:	tstugt == hi
condt:	tstuge == pl

condf:	tstoeq == ne
condf:	tstune == eq
condf:	tstolt == pl
condf:	tstole == hi
condf:	tstogt == le
condf:	tstoge == lt
